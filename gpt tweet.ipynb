{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e56f7b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from time import sleep\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8dad39d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Service(\"C:/Users/OWNER/Desktop/delenium/chromedriver.exe\")\n",
    "driver = webdriver.Chrome(service=s)\n",
    "driver.get(\"https://twitter.com/home\")\n",
    "time.sleep(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "571da802",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = driver.find_element(By.XPATH,'//input[@name=\"text\"]')\n",
    "username.send_keys(\"victorolanikanju@gmail.com\")\n",
    "time.sleep(10)\n",
    "username.send_keys(Keys.ENTER)\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d3925cb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "password=driver.find_element(By.XPATH,'//input[@name=\"password\"]')\n",
    "time.sleep(5)\n",
    "password.send_keys(\"Olanikanju\")\n",
    "time.sleep(10)\n",
    "password.send_keys(Keys.ENTER)\n",
    "time.sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3ed51e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "search_input = driver.find_element(By.XPATH,'//input[@aria-label=\"Search query\"]')\n",
    "time.sleep(10)\n",
    "search_input.send_keys(\"buhari\")\n",
    "time.sleep(10)\n",
    "search_input.send_keys(Keys.RETURN)\n",
    "time.sleep(5)\n",
    "#driver.find_element(By.LINK_TEXT,\"people\").click()\n",
    "#time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7cd8d86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element(By.XPATH,\"/html/body/div[1]/div/div/div[2]/main/div/div/div/div/div/div[1]/div[1]/div[2]/nav/div/div[2]/div/div[3]/a/div/div/span\").click()\n",
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "53c6992f",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element(By.XPATH,\"/html/body/div[1]/div/div/div[2]/main/div/div/div/div/div/div[3]/section/div/div/div/div/div[1]/div/div/div/div/div[2]/div[1]\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef864a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "UserTags=[]\n",
    "TimeStamps=[]\n",
    "Tweets=[]\n",
    "Replys=[]\n",
    "reTweets=[]\n",
    "Likes=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb0d0bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function to scroll to the bottom of the page\n",
    "def scroll_to_bottom():\n",
    "    # Get the current page height\n",
    "    current_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        # Scroll to the bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        # Wait for some time to load new content\n",
    "        time.sleep(2)\n",
    "        # Check if new content has loaded (compare with previous height)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == current_height:\n",
    "            break\n",
    "        current_height = new_height\n",
    "\n",
    "# Call the function to scroll to the bottom of the page\n",
    "scroll_to_bottom()\n",
    "\n",
    "# Now, you can start scraping the loaded content\n",
    "# You can use Selenium's find_element(s)_by_* methods to locate and extract the desired information\n",
    "\n",
    "# Example: Scraping user tags, timestamps, tweets, replies, retweets, and likes\n",
    "tweets = driver.find_elements(By.CSS_SELECTOR, '.tweet')\n",
    "\n",
    "#for tweet in tweets:\n",
    " #   user_tag = tweet.find_element(By.CSS_SELECTOR, '.username').get_attribute('textContent')\n",
    "  ##  timestamp = tweet.find_element(By.CSS_SELECTOR, 'time').get_attribute('datetime')\n",
    "    #tweet_text = tweet.find_element(By.CSS_SELECTOR, '.tweet-text').get_attribute('textContent')\n",
    "    #reply_count = tweet.find_element(By.CSS_SELECTOR, '.reply-count').get_attribute('textContent')\n",
    "    #retweet_count = tweet.find_element(By.CSS_SELECTOR, '.retweet-count').get_attribute('textContent')\n",
    "    #like_count = tweet.find_element(By.CSS_SELECTOR, '.like-count').get_attribute('textContent')\n",
    "\n",
    "    #UserTags.append(user_tag)\n",
    "    #TimeStamps.append(timestamp)\n",
    "    \n",
    "    #Tweets.append(tweet_text)\n",
    "    \n",
    "    #R\n",
    "#eplys.append(reply_count)\n",
    "    #reTweets.append(retweet_count)\n",
    "  #  Likes.append(like_count)\n",
    "\n",
    "# Close the driver when you're done\n",
    "driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9e58d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#( Print or use the scraped data as per your requirement\n",
    "for i in range(len(Tweets)):\n",
    "    print(f\"User Tag: {UserTags[i]}\")\n",
    "    print(f\"Timestamp: {TimeStamps[i]}\")\n",
    "    print(f\"Tweet: {Tweets[i]}\")\n",
    "    print(f\"Replies: {Replys[i]}\")\n",
    "    print(f\"Retweets: {reTweets[i]}\")\n",
    "    print(f\"Likes: {Likes[i]}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d868792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully to C:\\Users\\OWNER\\Desktop/twitter_data.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Your code for scraping the data (e.g., UserTags, TimeStamps, Tweets, Replys, reTweets, Likes)\n",
    "\n",
    "# Define the file path to save the CSV\n",
    "csv_file = r\"C:\\Users\\OWNER\\Desktop/twitter_data.csv\"\n",
    "\n",
    "\n",
    "# Combine the scraped data into a list of tuples\n",
    "data = list(zip(UserTags, TimeStamps, Tweets, Replys, reTweets, Likes))\n",
    "\n",
    "# Write the data to the CSV file\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write the header row\n",
    "    writer.writerow(['User Tag', 'Timestamp', 'Tweet', 'Replies', 'Retweets', 'Likes'])\n",
    "    # Write the data rows\n",
    "    writer.writerows(data)\n",
    "\n",
    "print(\"Data saved successfully to\", csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ee7b77cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in tweets:\n",
    "    user_tag = tweet.find_element(By.CSS_SELECTOR, '.username').get_attribute('textContent')\n",
    "    print(\"User Tag:\", user_tag)\n",
    "    # Continue with the rest of the scraping code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f23c79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
